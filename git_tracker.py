#!/usr/bin/env python3
"""
Git File Tracker for React Projects
T·ª± ƒë·ªông theo d√µi v√† c·∫≠p nh·∫≠t c√°c file theo lo·∫°i khi c√≥ commit m·ªõi
"""

import os
import subprocess
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Any # Added Any for errorDict typing
import argparse
import logging
import fnmatch
import sys
import math # Di chuy·ªÉn l√™n ƒë·∫ßu file
import re


core_files = [
    # Bootloader and common assets/styles
    "appengine/common/boot.js",
    "appengine/common/common.css",
    "appengine/common/prettify.css",
    "appengine/common/prettify.js",
    "appengine/common/back.js",
    
    # Core libraries for UI, logic, and integration
    "appengine/src/html.js",
    "appengine/src/lib-games.js",
    "appengine/src/lib-interface.js",
    "appengine/src/lib-dialogs.js",
    "appengine/src/lib-code.js",
    "appengine/src/lib-storage.js",
    "appengine/src/lib-ace.js",
    "appengine/src/lib-gallery.js",
    "appengine/src/slider.js",

    # Third-party and externs for JS execution environment
    "third-party/base.js",
    "externs/interpreter-externs.js",
    "externs/prettify-externs.js",
    "externs/soundJS-externs.js",
    "externs/storage-externs.js",
    "externs/svg-externs.js",

    # App Engine configuration
    "appengine/app.yaml"
]

# =================================================================
# ==                    GAME-SPECIFIC FILES                      ==
# =================================================================

# --- Game: Maze (M√™ cung) ---
maze_files = core_files + [
    "appengine/maze.html",
    "appengine/maze/style.css",
    "appengine/maze/src/main.js",
    "appengine/maze/src/blocks.js",
    "appengine/maze/src/html.js"
]

# --- Game: Bird (Ch√∫ chim) ---
bird_files = core_files + [
    "appengine/bird.html",
    "appengine/bird/style.css",
    "appengine/bird/src/main.js",
    "appengine/bird/src/blocks.js",
    "appengine/bird/src/html.js"
]

# --- Game: Turtle (R√πa v·∫Ω) ---
turtle_files = [
    "appengine/turtle.html",
    "appengine/turtle/style.css",
    "appengine/turtle/src/main.js",
    "appengine/turtle/src/blocks.js",
    "appengine/turtle/src/html.js"
]

# --- Game: Movie (L√†m phim) ---
movie_files = core_files + [
    "appengine/movie.html",
    "appengine/movie/style.css",
    "appengine/movie/src/main.js",
    "appengine/movie/src/blocks.js",
    "appengine/movie/src/html.js",
    "appengine/movie/src/scrubber.js"
]

# --- Game: Music (√Çm nh·∫°c) ---
music_files = core_files + [
    "appengine/music.html",
    "appengine/music/style.css",
    "appengine/music/src/main.js",
    "appengine/music/src/blocks.js",
    "appengine/music/src/html.js",
    "appengine/music/src/field_pitch.js",
    "appengine/music/src/startcount.js"
]

# --- Game: Puzzle (X·∫øp h√¨nh) ---
puzzle_files = core_files + [
    "appengine/puzzle.html",
    "appengine/puzzle/style.css",
    "appengine/puzzle/src/main.js",
    "appengine/puzzle/src/blocks.js",
    "appengine/puzzle/src/html.js",
    "appengine/puzzle/src/data.js"
]

# --- Game: Pond (Ao n∆∞·ªõc) ---
pond_files = core_files + [
    # Entry points
    "appengine/pond-tutor.html",
    "appengine/pond-duck.html",
    "appengine/pond/docs.html",
    
    # Core Pond logic
    "appengine/pond/src/pond.js",
    "appengine/pond/src/avatar.js",
    "appengine/pond/src/battle.js",
    "appengine/pond/src/visualization.js",
    "appengine/pond/src/blocks.js",
    "appengine/pond/src/js-blocks.js",
    "appengine/pond/src/html.js",
    "appengine/pond/style.css",

    # Tutor specific
    "appengine/pond/tutor/style.css",
    "appengine/pond/tutor/src/main.js",
    "appengine/pond/tutor/src/html.js",

    # Duck specific
    "appengine/pond/duck/style.css",
    "appengine/pond/duck/src/main.js",
    "appengine/pond/duck/src/html.js",
    "appengine/pond/duck/default-ducks.js",
    
    # Docs specific
    "appengine/pond/docs/style.css",
    "appengine/pond/docs/docs.js"
]

# --- Gallery (Th∆∞ vi·ªán) ---
gallery_files = core_files + [
    "appengine/gallery.html",
    "appengine/gallery/style.css",
    "appengine/gallery/src/main.js",
    "appengine/gallery/src/html.js",
    # API backend files
    "appengine/gallery_api/admin.py",
    "appengine/gallery_api/common.py",
    "appengine/gallery_api/expire.py",
    "appengine/gallery_api/submit.py",
    "appengine/gallery_api/view.py"
]

# --- Index (Trang ch·ªß) ---
index_files = core_files + [
    "appengine/index.html",
    "appengine/index/style.css",
    "appengine/index/src/main.js",
    "appengine/index/src/html.js"
]


# ===== KHAI B√ÅO BI·∫æN fileList TO√ÄN C·ª§C =====
fileList: List[str] = []



# [
#   "packages/interactive-quiz-kit/src/client-services.ts",
#   "packages/interactive-quiz-kit/src/react-ui.ts",
#   "packages/interactive-quiz-kit/src/ai.ts",
#   "packages/interactive-quiz-kit/src/types.ts",
#   "packages/interactive-quiz-kit/src/index.ts",
#   "packages/interactive-quiz-kit/src/i18n.ts",
#   "packages/interactive-quiz-kit/src/services/PointAllocationService.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-learning-analysis.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-learning-analysis-types.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-practice-suggestion.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-questions-from-quiz-plan-types.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-quiz-plan-types.ts",
#   "packages/interactive-quiz-kit/src/ai/flows/generate-single-knowledge-card-types.ts"
# ];

# [
#     # 1. UI - N∆°i b·∫Øt ƒë·∫ßu h√†nh ƒë·ªông (Click n√∫t "T·∫°o M·ªõi")
#     "features/core-gameplay/components/Sandbox/LevelManagementPanel.tsx",

#     # 2. Page/View - N∆°i ƒëi·ªÅu ph·ªëi c√°c component UI v√† g·ªçi logic
#     "features/core-gameplay/components/Sandbox/SandboxView.tsx",

#     # 3. Controller - N∆°i ch·ª©a logic nghi·ªáp v·ª• cho ch·∫ø ƒë·ªô ch·ªânh s·ª≠a
#     "features/core-gameplay/components/EditMode/EditModeController.tsx",
    
#     # 4. State & Services - C√°c l·ªõp qu·∫£n l√Ω tr·∫°ng th√°i v√† d·ªØ li·ªáu c·ªët l√µi
#     "core/state/ApplicationStateManager.ts",
#     "services/domains/gameplay/WorldStateService.ts",
#     "services/domains/gameplay/LevelRepository.ts",

#     # 5. Rendering - C√°c component ch·ªãu tr√°ch nhi·ªám v·∫Ω l·∫°i th·∫ø gi·ªõi 3D d·ª±a tr√™n state
#     "features/core-gameplay/game-renderer/r3f-components/LevelScene.tsx",
    
#     # 6. Global Setup - N∆°i kh·ªüi t·∫°o v√† cung c·∫•p c√°c services
#     "App.tsx",
    
#     # 7. Data Definitions - N∆°i ƒë·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu
#     "shared/types/index.ts"
# ]


# [
#     "services/domains/gameplay/RobotController.ts",
#     "services/domains/gameplay/WorldStateService.ts",
#     "features/core-gameplay/components/PlayMode/GameplayView.tsx",
#     "features/core-gameplay/components/Sandbox/SandboxView.tsx",
#     "features/core-gameplay/components/EditMode/EditModeView.tsx",
#     "features/core-gameplay/components/EditMode/EditModeController.tsx",
#     "features/core-gameplay/components/PlayMode/GameControls/GameControls.tsx",
#     "features/core-gameplay/game-renderer/config/gameAssets.ts",
#     "shared/types/state.ts",
#     "shared/types/index.ts",
#     "core/state/ApplicationStateManager.ts",
# ]

# [
#   "assets/styles/global.css",
#   "assets/styles/theme.css",
#   "layouts/AdminLayout.css",
#   "layouts/AuthenticatedLayout.css",
#   "layouts/PublicLayout.css",
#   "features/authentication/components/LoginPage.css",
#   "features/competition/components/Leaderboard.css",
#   "features/competition/components/Timer.css",
#   "features/core-gameplay/components/PlayMode/GameHUD.css",
#   "features/gamification/components/AchievementSystem/AchievementSystem.css",
#   "shared/components/LoadingSpinner.css"
# ]





# ============================================

class GitFileTracker:
    def __init__(self, project_path: str, output_dir: str = "tracked_files"):
        self.project_path = Path(project_path).resolve()
        self.output_dir = self.project_path / output_dir
        self.output_dir.mkdir(exist_ok=True)

        self.file_types = {
            'typescript': ['.ts', '.tsx'],
            'javascript': ['.js', '.jsx'],
            # Th√™m c√°c lo·∫°i file kh√°c n·∫øu c·∫ßn
        }

        self.ignore_patterns: Set[str] = {
            'node_modules/', 'dist/', 'build/', '.git/', '.vscode/',
            '.idea/', 'coverage/', '.nyc_output/', '.cache/',
            '.DS_Store', '*.log',
            '.env.local', '*.env.local', '.env.*.local',
            'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
        }

        self.tsconfig_cache: Dict[Path, Dict] = {} 
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'tracker.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

        self.metadata_file = self.output_dir / 'metadata.json'
        self.load_metadata()

    def load_metadata(self):
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            except json.JSONDecodeError:
                self.logger.error(f"Error decoding JSON from {self.metadata_file}. Initializing new metadata.")
                self._initialize_metadata()
        else:
            self._initialize_metadata()

    def _initialize_metadata(self):
        self.metadata = {
            'last_commit': None,
            'tracked_files': {},
            'file_hashes': {},
            'created': datetime.now().isoformat()
        }

    def save_metadata(self):
        self.metadata['updated'] = datetime.now().isoformat()
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)

    def get_current_commit(self) -> str | None: # Python 3.10+ union type
        try:
            result = subprocess.run(
                ['git', 'rev-parse', 'HEAD'],
                cwd=self.project_path,
                capture_output=True,
                text=True,
                check=True,
                encoding='utf-8'
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Kh√¥ng th·ªÉ l·∫•y commit hash hi·ªán t·∫°i: {e}")
            return None
        except FileNotFoundError:
            self.logger.error("L·ªánh 'git' kh√¥ng t√¨m th·∫•y. H√£y ƒë·∫£m b·∫£o Git ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† c√≥ trong PATH.")
            return None


    def get_tracked_files(self) -> List[str]:
        try:
            result = subprocess.run(
                ['git', 'ls-files'],
                cwd=self.project_path,
                capture_output=True,
                text=True,
                check=True,
                encoding='utf-8'
            )
            files = result.stdout.strip().split('\n')
            return [f for f in files if f and not self.should_ignore_file(f)]
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Kh√¥ng th·ªÉ l·∫•y danh s√°ch file tracked: {e}")
            return []
        except FileNotFoundError:
            self.logger.error("L·ªánh 'git' kh√¥ng t√¨m th·∫•y khi l·∫•y danh s√°ch file.")
            return []

    def get_changed_files(self, since_commit: str | None = None) -> List[str]:
        try:
            if since_commit:
                cmd = ['git', 'diff', '--name-only', f'{since_commit}..HEAD']
            else:
                # L·∫•y c√°c file ƒë√£ thay ƒë·ªïi v√† ƒë∆∞·ª£c staged (ch∆∞a commit)
                cmd = ['git', 'diff', '--name-only', '--cached']

            result = subprocess.run(
                cmd,
                cwd=self.project_path,
                capture_output=True,
                text=True,
                check=True, # s·∫Ω raise error n·∫øu git diff tr·∫£ v·ªÅ non-zero (v√≠ d·ª•, commit hash kh√¥ng t·ªìn t·∫°i)
                encoding='utf-8'
            )
            files = result.stdout.strip().split('\n')
            return [f for f in files if f and not self.should_ignore_file(f)]
        except subprocess.CalledProcessError as e:
            # ƒê√¢y c√≥ th·ªÉ l√† tr∆∞·ªùng h·ª£p b√¨nh th∆∞·ªùng n·∫øu kh√¥ng c√≥ thay ƒë·ªïi, ho·∫∑c commit hash sai
            self.logger.warning(f"Kh√¥ng th·ªÉ l·∫•y danh s√°ch file ƒë√£ thay ƒë·ªïi (since {since_commit}): {e}. ƒêi·ªÅu n√†y c√≥ th·ªÉ b√¨nh th∆∞·ªùng n·∫øu kh√¥ng c√≥ thay ƒë·ªïi ho·∫∑c commit hash kh√¥ng h·ª£p l·ªá.")
            return []
        except FileNotFoundError:
            self.logger.error("L·ªánh 'git' kh√¥ng t√¨m th·∫•y khi l·∫•y file thay ƒë·ªïi.")
            return []


    def should_ignore_file(self, file_path: str) -> bool:
        path_obj = Path(file_path)
        normalized_path_str = str(path_obj).replace('\\', '/') # Chu·∫©n h√≥a cho Windows

        for pattern in self.ignore_patterns:
            if pattern.endswith('/'): # Th∆∞ m·ª•c
                if normalized_path_str.startswith(pattern):
                    return True
            else: # File pattern
                if fnmatch.fnmatch(path_obj.name, pattern):
                    return True
        return False

    def get_file_type(self, file_path: str) -> str:
        p_file_path = Path(file_path)
        file_ext = p_file_path.suffix.lower()
        # file_name = p_file_path.name.lower() # Kh√¥ng c·∫ßn thi·∫øt n·∫øu ch·ªâ d·ª±a v√†o extension

        for type_name, extensions in self.file_types.items():
            if file_ext in extensions:
                return type_name
        # M·∫∑c ƒë·ªãnh cho c√°c file kh√¥ng kh·ªõp
        if file_ext == '.css': return 'styles'
        if file_ext == '.json': return 'config'
        if file_ext == '.md': return 'markdown'
        if file_ext == '.html': return 'html'
        if file_ext in ['.svg', '.png', '.jpg', '.jpeg', '.gif', '.glb', '.gltf']: return 'assets'

        return 'other'


    def calculate_file_hash(self, file_path: Path) -> str | None:
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except FileNotFoundError:
            self.logger.warning(f"File not found for hashing: {file_path}")
            return None
        except Exception as e:
            self.logger.error(f"Error hashing file {file_path}: {e}")
            return None

    def read_file_content(self, file_path: Path) -> str:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='latin-1') as f: # Th·ª≠ encoding kh√°c
                    return f.read()
            except Exception as e_latin1:
                self.logger.warning(f"Kh√¥ng th·ªÉ ƒë·ªçc file (UTF-8 and Latin-1 failed): {file_path} - {e_latin1}")
                return f"# FAILED_TO_READ_FILE_CONTENT (encoding issue): {file_path.name}\n"
        except FileNotFoundError:
            self.logger.warning(f"File not found for reading content: {file_path}")
            return f"# FILE_NOT_FOUND: {file_path.name}\n"
        except Exception as e:
            self.logger.error(f"L·ªói ƒë·ªçc file: {file_path} - {e}")
            return f"# ERROR_READING_FILE: {file_path.name}\n"

    # <<<<<<<<<<<<<<<< FIX HERE: H√†m ƒë√£ ƒë∆∞·ª£c un-indent ƒë·ªÉ tr·ªü th√†nh m·ªôt method c·ªßa class >>>>>>>>>>>>>>>>
    def create_consolidated_file(self, file_type: str, files: List[str]):
        output_file = self.output_dir / f"{file_type}_files.txt" # ƒê·ªïi th√†nh .txt cho d·ªÖ ƒë·ªçc
        content = [
            f"# Consolidated {file_type.upper()} Files",
            f"# Generated: {datetime.now().isoformat()}",
            f"# Total files: {len(files)}",
            "=" * 80, ""
        ]

        for file_path_str in sorted(files): # S·∫Øp x·∫øp ƒë·ªÉ output nh·∫•t qu√°n
            full_path = self.project_path / file_path_str
            if full_path.exists() and full_path.is_file():
                normalized_file_path_str = file_path_str.replace('\\', '/')
                content.extend([
                    f"# FILE: {normalized_file_path_str}", # S·ª≠ d·ª•ng bi·∫øn ƒë√£ chu·∫©n h√≥a
                    "-" * 60,
                    self.read_file_content(full_path),
                    "", "=" * 80, ""
                ])
            else:
                self.logger.warning(f"Skipping non-existent file in consolidated report: {full_path}")

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(content))
        self.logger.info(f"T·∫°o file t·ªïng h·ª£p: {output_file} ({len(files)} files)")

    def update_consolidated_files(self, changed_files_paths: List[str]):
        types_affected: Set[str] = set()
        for file_path in changed_files_paths:
            types_affected.add(self.get_file_type(file_path))

        for file_type in types_affected:
            all_current_files_of_type = self.get_files_by_type(file_type)
            if all_current_files_of_type:
                self.create_consolidated_file(file_type, all_current_files_of_type)
            else: # N·∫øu kh√¥ng c√≤n file n√†o c·ªßa lo·∫°i n√†y
                consolidated_file_path = self.output_dir / f"{file_type}_files.txt"
                if consolidated_file_path.exists():
                    try:
                        consolidated_file_path.unlink()
                        self.logger.info(f"ƒê√£ x√≥a file t·ªïng h·ª£p (kh√¥ng c√≤n file lo·∫°i n√†y): {consolidated_file_path}")
                    except OSError as e:
                        self.logger.error(f"Kh√¥ng th·ªÉ x√≥a file t·ªïng h·ª£p {consolidated_file_path}: {e}")


    def get_files_by_type(self, target_type: str) -> List[str]:
        all_files = self.get_tracked_files() # L·∫•y danh s√°ch file m·ªõi nh·∫•t t·ª´ git
        return [f for f in all_files if self.get_file_type(f) == target_type]

    def create_project_structure(self):
        structure_file = self.output_dir / "project_structure.txt"
        content = [
            "# Project Structure",
            f"# Generated: {datetime.now().isoformat()}",
            f"# Project Path: {self.project_path}",
            "=" * 80, ""
        ]
        content.extend(self.generate_tree_structure())
        content.extend(["", "=" * 80, "# STATISTICS", "=" * 80])
        content.extend(self.get_project_statistics())

        with open(structure_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(content))
        self.logger.info(f"T·∫°o file c·∫•u tr√∫c d·ª± √°n: {structure_file}")

    def generate_tree_structure(self) -> List[str]:
        tree_lines = []
        # L·∫•y danh s√°ch file m·ªõi nh·∫•t t·ª´ git ƒë·ªÉ x√¢y d·ª±ng c√¢y th∆∞ m·ª•c
        tracked_files_for_tree = self.get_tracked_files()
        file_tree = {}

        for file_path_str in tracked_files_for_tree:
            parts = Path(file_path_str).parts
            current_level = file_tree
            for i, part in enumerate(parts):
                is_file_node = (i == len(parts) - 1)
                if part not in current_level:
                    current_level[part] = {'_is_file': is_file_node}
                    if is_file_node:
                        # L∆∞u lo·∫°i file ƒë·ªÉ hi·ªÉn th·ªã indicator
                        current_level[part]['_type'] = self.get_file_type(file_path_str)
                current_level = current_level[part]

        # B·∫Øt ƒë·∫ßu x√¢y d·ª±ng t·ª´ th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
        tree_lines.append(f"{self.project_path.name}/")
        self._build_tree_recursive(file_tree, tree_lines, "", True) # True cho is_root_level
        return tree_lines


    def _build_tree_recursive(self, node: dict, lines: List[str], prefix: str, is_root_level: bool = False):
        # S·∫Øp x·∫øp: th∆∞ m·ª•c tr∆∞·ªõc, file sau, r·ªìi theo t√™n
        items = sorted(
            [(k, v) for k, v in node.items() if not k.startswith('_')], # B·ªè qua c√°c key n·ªôi b·ªô nh∆∞ '_is_file'
            key=lambda x: (x[1].get('_is_file', False), x[0].lower()) # S·∫Øp x·∫øp th∆∞ m·ª•c (False) tr∆∞·ªõc file (True)
        )

        for i, (name, value) in enumerate(items):
            is_last_item = (i == len(items) - 1)
            connector = "‚îî‚îÄ‚îÄ " if is_last_item else "‚îú‚îÄ‚îÄ "
            current_line_prefix = prefix + connector

            # Prefix cho c√°c d√≤ng con b√™n trong th∆∞ m·ª•c n√†y
            children_prefix = prefix + ("    " if is_last_item else "‚îÇ   ")

            if value.get('_is_file', False): # L√† file
                file_type_str = value.get('_type', 'other')
                type_indicator_str = self._get_file_type_indicator(file_type_str)
                lines.append(f"{current_line_prefix}{name} {type_indicator_str}")
            else: # L√† th∆∞ m·ª•c
                lines.append(f"{current_line_prefix}{name}/")
                self._build_tree_recursive(value, lines, children_prefix, False)

    def _load_tsconfig(self, start_path: Path) -> tuple[Dict, Path] | None:
        """
        T√¨m v√† parse file tsconfig.json g·∫ßn nh·∫•t, ƒëi ng∆∞·ª£c t·ª´ th∆∞ m·ª•c b·∫Øt ƒë·∫ßu.
        Tr·∫£ v·ªÅ m·ªôt tuple (d·ªØ li·ªáu, ƒë∆∞·ªùng d·∫´n file) ho·∫∑c None.
        K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c cache l·∫°i ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô.
        """
        current_dir = start_path if start_path.is_dir() else start_path.parent
        
        # ƒêi ng∆∞·ª£c t·ª´ th∆∞ m·ª•c hi·ªán t·∫°i l√™n th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
        while self.project_path in current_dir.parents or current_dir == self.project_path:
            tsconfig_path = current_dir / "tsconfig.json"
            
            if tsconfig_path in self.tsconfig_cache:
                cached_data = self.tsconfig_cache[tsconfig_path]
                # Tr·∫£ v·ªÅ t·ª´ cache n·∫øu n√≥ kh√¥ng ph·∫£i l√† marker l·ªói
                return (cached_data, tsconfig_path) if cached_data else None

            if tsconfig_path.is_file():
                try:
                    with open(tsconfig_path, 'r', encoding='utf-8') as f:
                        # B·ªè qua c√°c comment trong file JSON tr∆∞·ªõc khi parse
                        content = re.sub(r'//.*', '', f.read(), flags=re.MULTILINE)
                        data = json.loads(content)
                        self.tsconfig_cache[tsconfig_path] = data
                        return data, tsconfig_path
                except Exception as e:
                    self.logger.warning(f"L·ªói khi ƒë·ªçc ho·∫∑c parse {tsconfig_path}: {e}")
                    self.tsconfig_cache[tsconfig_path] = {} # Cache l·ªói ƒë·ªÉ kh√¥ng th·ª≠ l·∫°i
                    return None
            
            if current_dir == current_dir.parent: # ƒê√£ ƒë·∫øn g·ªëc h·ªá th·ªëng
                break
            current_dir = current_dir.parent
        
        return None

    def _resolve_import_path(self, importer_path: Path, import_str: str) -> Path | None:
        """
        Gi·∫£i quy·∫øt m·ªôt chu·ªói import th√†nh ƒë∆∞·ªùng d·∫´n file th·ª±c t·∫ø, c√≥ h·ªó tr·ª£ tsconfig paths.
        """
        # 1. X·ª≠ l√Ω import t∆∞∆°ng ƒë·ªëi (∆∞u ti√™n cao nh·∫•t v√† nhanh nh·∫•t)
        if import_str.startswith('.'):
            base_dir = importer_path.parent
            potential_path = (base_dir / import_str).resolve()
            resolved = self._find_file_with_extension(potential_path)
            if resolved:
                return resolved

        # 2. X·ª≠ l√Ω path aliases t·ª´ tsconfig.json
        tsconfig_result = self._load_tsconfig(importer_path)
        if tsconfig_result:
            tsconfig_data, tsconfig_path = tsconfig_result
            tsconfig_dir = tsconfig_path.parent # <-- ƒê√ÇY L√Ä S·ª¨A L·ªñI QUAN TR·ªåNG

            if 'compilerOptions' in tsconfig_data:
                options = tsconfig_data['compilerOptions']
                base_url_str = options.get('baseUrl', '.')
                base_url = (tsconfig_dir / base_url_str).resolve()
                
                paths = options.get('paths', {})
                for alias, real_paths in paths.items():
                    pattern_str = re.escape(alias).replace(r'\*', r'(.*)')
                    if not pattern_str.endswith('$'):
                         pattern_str += '$'
                    
                    match = re.match(pattern_str, import_str)
                    if match:
                        captured_part = match.group(1) if match.groups() else ""
                        for real_path_template in real_paths:
                            real_path_str = real_path_template.replace('*', captured_part, 1)
                            potential_path = (base_url / real_path_str).resolve()
                            resolved = self._find_file_with_extension(potential_path)
                            if resolved:
                                return resolved

        # 3. X·ª≠ l√Ω c√°c import tuy·ªát ƒë·ªëi t·ª´ g·ªëc project (fallback)
        potential_path_from_root = (self.project_path / import_str).resolve()
        resolved_from_root = self._find_file_with_extension(potential_path_from_root)
        if resolved_from_root:
            return resolved_from_root
            
        return None


    def _find_file_with_extension(self, potential_path: Path) -> Path | None:
        """
        T√¨m file th·ª±c t·∫ø b·∫±ng c√°ch th·ª≠ c√°c extension ph·ªï bi·∫øn (.ts, .tsx, .js, /index.ts, etc.)
        (H√ÄM N√ÄY GI·ªÆ NGUY√äN)
        """
        extensions_to_try = ['.ts', '.tsx', '.js', '.jsx', '.json']
        # 1. Th·ª≠ v·ªõi ƒë∆∞·ªùng d·∫´n g·ªëc (n·∫øu n√≥ ƒë√£ c√≥ extension)
        if potential_path.exists() and potential_path.is_file():
            return potential_path

        # 2. Th·ª≠ th√™m c√°c extension
        for ext in extensions_to_try:
            path_with_ext = potential_path.with_suffix(ext)
            if path_with_ext.exists() and path_with_ext.is_file():
                return path_with_ext

        # 3. Th·ª≠ tr∆∞·ªùng h·ª£p l√† th∆∞ m·ª•c (import file index)
        if potential_path.is_dir():
            for ext in extensions_to_try:
                # ƒê·∫∑c bi·ªát ∆∞u ti√™n index.ts/tsx/js
                index_file = potential_path / f"index{ext}"
                if index_file.exists() and index_file.is_file():
                    return index_file
                # M·ªôt s·ªë package export ch√≠nh n√≥, v√≠ d·ª•: 'packages/interactive-quiz-kit/src/react-ui' -> react-ui.ts
                self_file = potential_path.parent / f"{potential_path.name}{ext}"
                if self_file.exists() and self_file.is_file():
                    return self_file
        
        return None

    def _get_file_type_indicator(self, file_type: str) -> str:
        indicators = {
            'typescript': 'üîπ', 'javascript': 'üü®', 'styles': 'üé®',
            'config': '‚öôÔ∏è', 'markdown': 'üìù', 'html': 'üåê',
            'assets': 'üñºÔ∏è', 'other': 'üìÑ'
            # Th√™m c√°c indicator kh√°c n·∫øu c·∫ßn
        }
        return indicators.get(file_type, 'üìÑ') # M·∫∑c ƒë·ªãnh l√† 'other'


    def get_project_statistics(self) -> List[str]:
        tracked_files = self.get_tracked_files() # L·∫•y danh s√°ch file m·ªõi nh·∫•t
        stats = [f"Total tracked files (respecting .gitignore & script ignores): {len(tracked_files)}"]

        files_by_type_counts: Dict[str, int] = {}
        total_size = 0

        for file_path_str in tracked_files:
            file_type = self.get_file_type(file_path_str)
            files_by_type_counts[file_type] = files_by_type_counts.get(file_type, 0) + 1
            full_path = self.project_path / file_path_str
            try:
                if full_path.is_file():
                    total_size += full_path.stat().st_size
            except FileNotFoundError:
                 self.logger.warning(f"File not found during stat calculation: {full_path}")
            except Exception as e:
                 self.logger.error(f"Error getting size for {full_path}: {e}")


        stats.append(f"Total size: {self._format_size(total_size)}")
        stats.extend(["", "Files by type:"])

        if tracked_files: # Ch·ªâ t√≠nh % n·∫øu c√≥ file
            for file_type_key in sorted(files_by_type_counts.keys()):
                count = files_by_type_counts[file_type_key]
                percentage = (count / len(tracked_files)) * 100
                indicator = self._get_file_type_indicator(file_type_key)
                stats.append(f"  {indicator} {file_type_key.capitalize()}: {count} files ({percentage:.1f}%)")
        else:
            stats.append("  No files to analyze.")


        stats.extend(["", "Directory statistics (top 10 by file count in top-level dirs):"])
        dir_stats = self._get_directory_stats(tracked_files)
        for dir_name, count in sorted(dir_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
            stats.append(f"  üìÅ {dir_name}: {count} files")

        return stats

    def _format_size(self, size_bytes: int) -> str:
        if size_bytes < 0: size_bytes = 0 # Handle potential negative (though unlikely)
        if size_bytes == 0: return "0B"
        size_names = ["B", "KB", "MB", "GB", "TB"]
        i = 0
        if size_bytes > 0: # ƒê·∫£m b·∫£o log kh√¥ng l·ªói n·∫øu size_bytes l√† 0
            i = int(math.floor(math.log(size_bytes, 1024)))
            i = min(i, len(size_names) - 1) # Tr√°nh index out of bounds
        p = math.pow(1024, i)
        s = round(size_bytes / p, 2)
        return f"{s} {size_names[i]}"

    def _get_directory_stats(self, files: List[str]) -> Dict[str, int]:
        dir_stats: Dict[str, int] = {}
        for file_path_str in files:
            path_obj = Path(file_path_str)
            # Ch·ªâ l·∫•y th∆∞ m·ª•c c·∫•p 1
            if path_obj.parts: # ƒê·∫£m b·∫£o path c√≥ √≠t nh·∫•t m·ªôt ph·∫ßn
                top_level_dir = path_obj.parts[0]
                if top_level_dir: # ƒê·∫£m b·∫£o kh√¥ng ph·∫£i chu·ªói r·ªóng
                    dir_stats[top_level_dir] = dir_stats.get(top_level_dir, 0) + 1
        return dir_stats

    def initial_scan(self):
        self.logger.info("B·∫Øt ƒë·∫ßu scan ban ƒë·∫ßu...")
        all_tracked_files = self.get_tracked_files()
        files_by_type_map: Dict[str, List[str]] = {}

        for file_path_str in all_tracked_files:
            file_type = self.get_file_type(file_path_str)
            files_by_type_map.setdefault(file_type, []).append(file_path_str)

        for file_type, files_list in files_by_type_map.items():
            if files_list: self.create_consolidated_file(file_type, files_list)

        self.create_project_structure()

        current_commit_hash = self.get_current_commit()
        self.metadata['last_commit'] = current_commit_hash
        self.metadata['tracked_files'] = files_by_type_map

        new_file_hashes = {}
        for file_path_str in all_tracked_files:
            full_path = self.project_path / file_path_str
            if full_path.is_file():
                hash_val = self.calculate_file_hash(full_path)
                if hash_val: new_file_hashes[file_path_str] = hash_val
        self.metadata['file_hashes'] = new_file_hashes

        self.save_metadata()
        self.logger.info(f"Ho√†n th√†nh scan ban ƒë·∫ßu. T·ªïng c·ªông: {len(all_tracked_files)} files tracked.")

    def check_and_update(self):
        self.logger.info("Ki·ªÉm tra thay ƒë·ªïi...")
        current_commit_hash = self.get_current_commit()
        if not current_commit_hash:
            self.logger.error("Kh√¥ng th·ªÉ l·∫•y commit hi·ªán t·∫°i. B·ªè qua c·∫≠p nh·∫≠t.")
            return

        last_known_commit = self.metadata.get('last_commit')
        # if not last_known_commit: # S·ª≠a: N·∫øu ch∆∞a c√≥ commit, lu√¥n th·ª±c hi·ªán initial_scan
        #     self.logger.info("Kh√¥ng c√≥ commit tr∆∞·ªõc ƒë√≥. Th·ª±c hi·ªán scan ban ƒë·∫ßu.")
        #     self.initial_scan()
        #     return
        # Thay v√†o ƒë√≥, ch·ªâ c·∫ßn ki·ªÉm tra xem commit c√≥ kh√°c kh√¥ng, ho·∫∑c n·∫øu last_known_commit l√† None
        run_full_scan_logic = False
        if not last_known_commit or last_known_commit != current_commit_hash:
            self.logger.info(f"Commit ƒë√£ thay ƒë·ªïi t·ª´ '{last_known_commit}' sang '{current_commit_hash}' ho·∫∑c ch∆∞a c√≥ commit tr∆∞·ªõc. S·∫Ω ki·ªÉm tra thay ƒë·ªïi.")
            run_full_scan_logic = True
        else:
            self.logger.info(f"Kh√¥ng c√≥ commit m·ªõi k·ªÉ t·ª´ {last_known_commit}. Ki·ªÉm tra thay ƒë·ªïi file th·ªß c√¥ng.")
            # V·∫´n ti·∫øp t·ª•c ƒë·ªÉ check hash file

        changed_via_git_diff = []
        if run_full_scan_logic and last_known_commit: # Ch·ªâ diff n·∫øu c√≥ commit tr∆∞·ªõc ƒë√≥ ƒë·ªÉ so s√°nh
             changed_via_git_diff = self.get_changed_files(last_known_commit)


        all_current_git_files = self.get_tracked_files()

        files_to_reprocess_content: Set[str] = set(changed_via_git_diff)
        current_file_hashes: Dict[str, str] = {}
        structure_changed = False

        # So s√°nh hash cho t·∫•t c·∫£ c√°c file hi·ªán t·∫°i
        for file_path_str in all_current_git_files:
            full_path = self.project_path / file_path_str
            if full_path.is_file():
                current_hash = self.calculate_file_hash(full_path)
                if current_hash:
                    current_file_hashes[file_path_str] = current_hash
                    # N·∫øu hash kh√°c ho·∫∑c file m·ªõi (ch∆∞a c√≥ trong metadata c≈©)
                    if self.metadata.get('file_hashes', {}).get(file_path_str) != current_hash:
                        files_to_reprocess_content.add(file_path_str)


        # X√°c ƒë·ªãnh file ƒë√£ b·ªã x√≥a (c√≥ trong hash c≈©, kh√¥ng c√≥ trong git files hi·ªán t·∫°i)
        deleted_files_paths = set(self.metadata.get('file_hashes', {}).keys()) - set(all_current_git_files)
        if deleted_files_paths:
            self.logger.info(f"Ph√°t hi·ªán {len(deleted_files_paths)} file ƒë√£ b·ªã x√≥a: {', '.join(deleted_files_paths)}")
            structure_changed = True
            for dfp in deleted_files_paths:
                 types_affected_by_deletion = self.get_file_type(dfp) # L·∫•y lo·∫°i file c·ªßa file ƒë√£ x√≥a
                 # Logic c·∫≠p nh·∫≠t c√°c file t·ªïng h·ª£p cho lo·∫°i file n√†y s·∫Ω n·∫±m ·ªü d∆∞·ªõi


        # X√°c ƒë·ªãnh file m·ªõi (c√≥ trong git files hi·ªán t·∫°i, kh√¥ng c√≥ trong hash c≈©)
        new_files_paths = set(all_current_git_files) - set(self.metadata.get('file_hashes', {}).keys())
        if new_files_paths:
            self.logger.info(f"Ph√°t hi·ªán {len(new_files_paths)} file m·ªõi: {', '.join(new_files_paths)}")
            structure_changed = True
            # files_to_reprocess_content ƒë√£ bao g·ªìm c√°c file n√†y


        if not files_to_reprocess_content and not structure_changed :
            self.logger.info("Kh√¥ng c√≥ file n√†o thay ƒë·ªïi n·ªôi dung ho·∫∑c c·∫•u tr√∫c quan tr·ªçng.")
        else:
            self.logger.info(f"X·ª≠ l√Ω {len(files_to_reprocess_content)} file (thay ƒë·ªïi, m·ªõi) v√† {len(deleted_files_paths)} file ƒë√£ x√≥a.")

            # C·∫≠p nh·∫≠t danh s√°ch file theo lo·∫°i trong metadata
            current_files_by_type_map: Dict[str, List[str]] = {}
            for file_path_str in all_current_git_files:
                file_type = self.get_file_type(file_path_str)
                current_files_by_type_map.setdefault(file_type, []).append(file_path_str)
            self.metadata['tracked_files'] = current_files_by_type_map

            # X√°c ƒë·ªãnh c√°c lo·∫°i file b·ªã ·∫£nh h∆∞·ªüng b·ªüi thay ƒë·ªïi n·ªôi dung ho·∫∑c x√≥a
            types_affected: Set[str] = set()
            for f_path in files_to_reprocess_content: # Bao g·ªìm file m·ªõi, file thay ƒë·ªïi
                types_affected.add(self.get_file_type(f_path))
            for f_path_deleted in deleted_files_paths: # V√† lo·∫°i file c·ªßa c√°c file ƒë√£ x√≥a
                types_affected.add(self.get_file_type(f_path_deleted))


            # T·∫°o l·∫°i c√°c file t·ªïng h·ª£p cho c√°c lo·∫°i b·ªã ·∫£nh h∆∞·ªüng
            for file_type in types_affected:
                files_of_this_type_now = current_files_by_type_map.get(file_type, [])
                if files_of_this_type_now:
                    self.create_consolidated_file(file_type, files_of_this_type_now)
                else: # Kh√¥ng c√≤n file n√†o c·ªßa lo·∫°i n√†y
                    consolidated_file_path = self.output_dir / f"{file_type}_files.txt"
                    if consolidated_file_path.exists():
                        consolidated_file_path.unlink(missing_ok=True) # X√≥a file n·∫øu kh√¥ng c√≤n file lo·∫°i ƒë√≥
                        self.logger.info(f"ƒê√£ x√≥a file t·ªïng h·ª£p (kh√¥ng c√≤n file lo·∫°i n√†y): {consolidated_file_path}")

            self.metadata['file_hashes'] = current_file_hashes # C·∫≠p nh·∫≠t hash m·ªõi
            if structure_changed or files_to_reprocess_content: # C·∫≠p nh·∫≠t c·∫•u tr√∫c n·∫øu c·∫ßn
                self.create_project_structure()

        self.metadata['last_commit'] = current_commit_hash
        self.save_metadata()
        self.logger.info(f"Ho√†n th√†nh c·∫≠p nh·∫≠t. Commit hi·ªán t·∫°i: {current_commit_hash}")


    def merge_specific_files(self, file_list_to_merge: List[str], output_filename: str = "files-merged.txt"):
        if not file_list_to_merge:
            self.logger.warning("Danh s√°ch file ƒë·ªÉ merge r·ªóng. Kh√¥ng c√≥ h√†nh ƒë·ªông n√†o ƒë∆∞·ª£c th·ª±c hi·ªán.")
            return

        self.logger.info(f"B·∫Øt ƒë·∫ßu g·ªôp {len(file_list_to_merge)} file v√†o '{output_filename}'...")

        output_file = self.output_dir / output_filename
        content = [
            f"# Merged Files",
            f"# Generated: {datetime.now().isoformat()}",
            f"# Total files merged: {len(file_list_to_merge)}",
            "=" * 80, ""
        ]

        valid_files_found = 0
        for file_path_str in file_list_to_merge:
            # T·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n trong file_list_to_merge ƒë·ªÅu l√† t∆∞∆°ng ƒë·ªëi so v·ªõi self.project_path
            full_path = self.project_path / file_path_str

            if full_path.exists() and full_path.is_file():
                valid_files_found += 1
                self.logger.debug(f"  -> ƒêang ƒë·ªçc file: {file_path_str}")
                normalized_file_path_str = file_path_str.replace('\\', '/')
                content.extend([
                    f"# FILE: {normalized_file_path_str}", # S·ª≠ d·ª•ng bi·∫øn ƒë√£ chu·∫©n h√≥a
                    "-" * 60,
                    self.read_file_content(full_path),
                    "", "=" * 80, ""
                ])
            else:
                warning_msg = f"B·ªè qua file kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng ph·∫£i l√† file: {file_path_str} (Ki·ªÉm tra t·∫°i: {full_path})"
                self.logger.warning(warning_msg)
                normalized_file_path_str = file_path_str.replace('\\', '/')
                content.extend([
                    f"# FILE: {normalized_file_path_str}", # S·ª≠ d·ª•ng bi·∫øn ƒë√£ chu·∫©n h√≥a
                    f"# Reason: Not found or not a regular file at checked path '{full_path}'",
                    "=" * 80, ""
                ])

        if valid_files_found == 0:
            self.logger.warning(f"Kh√¥ng t√¨m th·∫•y file h·ª£p l·ªá n√†o trong danh s√°ch cung c·∫•p ƒë·ªÉ g·ªôp v√†o '{output_filename}'. File g·ªôp s·∫Ω kh√¥ng ƒë∆∞·ª£c t·∫°o/c·∫≠p nh·∫≠t.")
            # Quy·∫øt ƒë·ªãnh xem c√≥ n√™n x√≥a file output c≈© n·∫øu kh√¥ng c√≥ file n√†o h·ª£p l·ªá
            # if output_file.exists():
            #     output_file.unlink()
            #     self.logger.info(f"ƒê√£ x√≥a file output c≈© '{output_filename}' do kh√¥ng c√≥ file h·ª£p l·ªá m·ªõi.")
            return


        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))
            self.logger.info(f"‚úÖ Ho√†n th√†nh! ƒê√£ g·ªôp th√†nh c√¥ng {valid_files_found} file v√†o: {output_file}")
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi ghi file g·ªôp '{output_file}': {e}")

    def merge_directory_files(self, dir_path_str: str, output_filename_base: str = "dir-merged"):
        target_dir = self.project_path / dir_path_str
        self.logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm file trong th∆∞ m·ª•c: '{target_dir}' ƒë·ªÉ g·ªôp...")

        if not target_dir.is_dir():
            self.logger.error(f"L·ªói: ƒê∆∞·ªùng d·∫´n '{dir_path_str}' kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng ph·∫£i l√† th∆∞ m·ª•c.")
            return

        files_to_merge_from_dir = []
        for path_obj in target_dir.rglob('*'):
            if path_obj.is_file():
                relative_path = path_obj.relative_to(self.project_path)
                relative_path_str = str(relative_path).replace('\\', '/')
                if not self.should_ignore_file(relative_path_str):
                    files_to_merge_from_dir.append(relative_path_str)
                else:
                    self.logger.debug(f"B·ªè qua file b·ªã ignore trong th∆∞ m·ª•c: {relative_path_str}")

        if not files_to_merge_from_dir:
            self.logger.warning(f"Kh√¥ng t√¨m th·∫•y file n√†o h·ª£p l·ªá ƒë·ªÉ g·ªôp trong th∆∞ m·ª•c '{dir_path_str}'.")
            return

        files_to_merge_from_dir.sort()
        output_filename = f"{output_filename_base}-{target_dir.name.replace(' ', '_')}.txt"
        self.merge_specific_files(files_to_merge_from_dir, output_filename=output_filename)

    # ===== START: CH·ª®C NƒÇNG MERGE THEO ERROR DICT =====
    def load_error_dict(self, error_dict_path_str: str) -> Dict[str, Any] | None:
        """ƒê·ªçc v√† parse file JSON errorDict."""
        error_dict_path = Path(error_dict_path_str)
        if not error_dict_path.is_file():
            self.logger.error(f"File errorDict kh√¥ng t√¨m th·∫•y t·∫°i: {error_dict_path}")
            return None
        try:
            with open(error_dict_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if "error_troubleshooting_map" not in data:
                self.logger.error(f"File errorDict '{error_dict_path}' kh√¥ng c√≥ key 'error_troubleshooting_map' ·ªü c·∫•p cao nh·∫•t.")
                return None
            return data
        except json.JSONDecodeError as e:
            self.logger.error(f"L·ªói parse JSON trong file errorDict '{error_dict_path}': {e}")
            return None
        except Exception as e:
            self.logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ƒë·ªçc file errorDict '{error_dict_path}': {e}")
            return None

    def merge_files_by_error_id(self, error_dict_data: Dict[str, Any], error_id: str):
        """
        G·ªôp c√°c file li√™n quan ƒë·∫øn m·ªôt ID l·ªói c·ª• th·ªÉ t·ª´ errorDict.
        """
        if not error_dict_data or "error_troubleshooting_map" not in error_dict_data:
            self.logger.error("D·ªØ li·ªáu errorDict kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu 'error_troubleshooting_map'.")
            return

        all_files_to_merge: Set[str] = set()
        error_item_found: Dict[str, Any] | None = None
        error_name_for_file = f"error-{error_id.replace('.', '_')}" # T√™n file m·∫∑c ƒë·ªãnh

        for category in error_dict_data.get("error_troubleshooting_map", []):
            if error_item_found: break
            for sub_category in category.get("sub_categories", []):
                if sub_category.get("id") == error_id:
                    error_item_found = sub_category
                    error_name_for_file = f"error-{category.get('category_id', 'cat')}_{error_id.replace('.', '_')}-{sub_category.get('name', 'unknown').replace(' ', '_').lower()[:30]}"
                    break
            # N·∫øu kh√¥ng t√¨m th·∫•y trong sub_categories, th·ª≠ t√¨m ·ªü category level (n·∫øu c·∫•u tr√∫c cho ph√©p)
            if not error_item_found and category.get("id") == error_id: # Gi·∫£ s·ª≠ category c≈©ng c√≥ th·ªÉ c√≥ "id"
                 error_item_found = category
                 error_name_for_file = f"error-cat_{error_id.replace('.', '_')}-{category.get('name', 'unknown').replace(' ', '_').lower()[:30]}"


        if not error_item_found:
            self.logger.error(f"Kh√¥ng t√¨m th·∫•y l·ªói v·ªõi ID '{error_id}' trong errorDict.")
            return

        self.logger.info(f"ƒê√£ t√¨m th·∫•y l·ªói: '{error_item_found.get('name', 'Kh√¥ng c√≥ t√™n')}' (ID: {error_id})")

        # Thu th·∫≠p file, ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n l√† t∆∞∆°ng ƒë·ªëi v·ªõi project_path
        # v√† chu·∫©n h√≥a v·ªÅ '/'
        for key in ["direct_files_frontend", "direct_files_backend", "indirect_files_frontend", "indirect_files_backend"]:
            for file_path in error_item_found.get(key, []):
                # Gi·∫£ ƒë·ªãnh c√°c ƒë∆∞·ªùng d·∫´n trong errorDict l√† t∆∞∆°ng ƒë·ªëi v·ªõi src/
                # N·∫øu kh√¥ng, b·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh logic n√†y.
                # V√≠ d·ª•: n·∫øu path l√† "features/auth/Login.tsx", n√≥ s·∫Ω th√†nh "src/features/auth/Login.tsx"
                # n·∫øu project_path c·ªßa b·∫°n l√† g·ªëc v√† errorDict d√πng path t·ª´ src.
                # Hi·ªán t·∫°i, gi·∫£ s·ª≠ path trong errorDict ƒë√£ l√† t∆∞∆°ng ƒë·ªëi so v·ªõi project_path
                # ho·∫∑c l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω.
                # Script n√†y mong ƒë·ª£i ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi so v·ªõi project_path
                # N·∫øu file JSON ch·ª©a path ki·ªÉu "src/...", th√¨ kh√¥ng c·∫ßn th√™m "src/" n·ªØa.
                # C√≤n n·∫øu file JSON ch·ªâ ch·ª©a "features/...", th√¨ b·∫°n c·∫ßn x√°c ƒë·ªãnh g·ªëc c·ªßa ch√∫ng.
                # Gi·∫£ s·ª≠ file JSON cung c·∫•p ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ th∆∞ m·ª•c `src`
                # v√† script n√†y ch·∫°y t·ª´ th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n (n∆°i c√≥ `src`).
                
                # Chuy·ªÉn ƒë·ªïi ƒë·ªÉ ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n l√† t∆∞∆°ng ƒë·ªëi v·ªõi g·ªëc d·ª± √°n
                # v√† s·ª≠ d·ª•ng / l√†m d·∫•u ph√¢n c√°ch
                # N·∫øu file_path ƒë√£ l√† "src/...", th√¨ kh√¥ng c·∫ßn l√†m g√¨ nhi·ªÅu.
                # N·∫øu file_path l√† "features/...", th√¨ c·∫ßn gh√©p v·ªõi "src/"
                
                # ƒê·ªÉ ƒë∆°n gi·∫£n, gi·∫£ s·ª≠ c√°c ƒë∆∞·ªùng d·∫´n trong JSON ƒë√£ chu·∫©n
                # v√† l√† t∆∞∆°ng ƒë·ªëi v·ªõi self.project_path
                # N·∫øu kh√¥ng, b·∫°n c·∫ßn chu·∫©n h√≥a ch√∫ng ·ªü ƒë√¢y.
                # V√≠ d·ª•, n·∫øu path trong JSON l√† "features/..." v√† b·∫°n mu·ªën n√≥ l√† "src/features/..."
                # standardized_path = Path("src") / file_path if not file_path.startswith("src/") else Path(file_path)
                # all_files_to_merge.add(str(standardized_path).replace('\\', '/'))
                
                # Hi·ªán t·∫°i, gi·ªØ nguy√™n:
                all_files_to_merge.add(str(Path(file_path)).replace('\\', '/'))


        if not all_files_to_merge:
            self.logger.warning(f"Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c li·ªát k√™ cho l·ªói ID '{error_id}'.")
            return

        sorted_files_list = sorted(list(all_files_to_merge))
        output_filename = f"{error_name_for_file}.merged.txt"
        self.merge_specific_files(sorted_files_list, output_filename=output_filename)
    # ===== END: CH·ª®C NƒÇNG MERGE THEO ERROR DICT =====


    def create_git_hook(self):
        git_dir = self.project_path / '.git'
        if not git_dir.is_dir():
            self.logger.error(f"Th∆∞ m·ª•c .git kh√¥ng t·ªìn t·∫°i t·∫°i {self.project_path}. Kh√¥ng th·ªÉ t·∫°o hook.")
            return

        hook_dir = git_dir / 'hooks'
        hook_dir.mkdir(exist_ok=True)
        hook_file = hook_dir / 'post-commit'
        script_path = Path(__file__).resolve() # L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa script n√†y

        # ƒê·∫£m b·∫£o c√°c ƒë∆∞·ªùng d·∫´n ƒë∆∞·ª£c truy·ªÅn v√†o hook l√† tuy·ªát ƒë·ªëi ho·∫∑c script bi·∫øt c√°ch t√¨m
        # S·ª≠ d·ª•ng sys.executable ƒë·ªÉ g·ªçi ƒë√∫ng interpreter Python
        hook_content = f"""#!/bin/sh
# Auto-generated git hook for file tracking by GitFileTracker
echo "GitFileTracker: Running post-commit hook..."
cd "{self.project_path}"
"{sys.executable}" "{script_path}" --project-path "{self.project_path}" --output-dir "{self.output_dir.name}" --check-update
echo "GitFileTracker: Post-commit hook finished."
"""
        try:
            with open(hook_file, 'w', encoding='utf-8') as f:
                f.write(hook_content)
            # ƒê·∫∑t quy·ªÅn th·ª±c thi cho hook file
            os.chmod(hook_file, 0o755) # rwxr-xr-x
            self.logger.info(f"ƒê√£ t·∫°o/c·∫≠p nh·∫≠t git hook: {hook_file}")
        except Exception as e:
            self.logger.error(f"Kh√¥ng th·ªÉ t·∫°o git hook: {e}")

    def _find_file_with_extension(self, potential_path: Path) -> Path | None:
        """
        T√¨m file th·ª±c t·∫ø b·∫±ng c√°ch th·ª≠ c√°c extension ph·ªï bi·∫øn (.ts, .tsx, .js, /index.ts, etc.)
        """
        extensions_to_try = ['.ts', '.tsx', '.js', '.jsx', '.json']
        # 1. Th·ª≠ v·ªõi ƒë∆∞·ªùng d·∫´n g·ªëc (n·∫øu n√≥ ƒë√£ c√≥ extension)
        if potential_path.exists() and potential_path.is_file():
            return potential_path

        # 2. Th·ª≠ th√™m c√°c extension
        for ext in extensions_to_try:
            path_with_ext = potential_path.with_suffix(ext)
            if path_with_ext.exists() and path_with_ext.is_file():
                return path_with_ext

        # 3. Th·ª≠ tr∆∞·ªùng h·ª£p l√† th∆∞ m·ª•c (import file index)
        if potential_path.is_dir():
            for ext in extensions_to_try:
                index_file = potential_path / f"index{ext}"
                if index_file.exists() and index_file.is_file():
                    return index_file
        
        # self.logger.debug(f"Kh√¥ng th·ªÉ gi·∫£i quy·∫øt ƒë∆∞·ªùng d·∫´n import: {potential_path}")
        return None

    def _extract_imports_from_file(self, file_path: Path) -> Set[Path]:
        """
        ƒê·ªçc m·ªôt file v√† tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c file n√≥ import.
        """
        resolved_imports: Set[Path] = set()
        if not file_path.exists() or not file_path.is_file():
            return resolved_imports

        content = self.read_file_content(file_path)
        
        # Regex ƒë·ªÉ t√¨m 'from "./path"' ho·∫∑c require("./path")
        # Bao g·ªìm c·∫£ d·∫•u ' v√† "
        import_regex = re.compile(r"(?:import|export)[\s\S]*?from\s*['\"](.*?)['\"]|require\s*\(\s*['\"](.*?)['\"]\s*\)")
        
        matches = import_regex.finditer(content)
        for match in matches:
            # Match.group(1) cho from '...', match.group(2) cho require('...')
            import_str = match.group(1) or match.group(2)
            if import_str:
                resolved_path = self._resolve_import_path(file_path, import_str)
                if resolved_path and resolved_path.is_file():
                    resolved_imports.add(resolved_path)

        return resolved_imports

    def _find_dependencies_recursively(self, start_file: Path, all_project_files_abs: Set[Path]) -> Set[Path]:
        """
        T√¨m t·∫•t c·∫£ c√°c file m√† start_file ph·ª• thu·ªôc v√†o, m·ªôt c√°ch ƒë·ªá quy.
        """
        to_visit = [start_file]
        visited: Set[Path] = set()

        while to_visit:
            current_file = to_visit.pop()
            if current_file in visited:
                continue
            
            # Ch·ªâ x·ª≠ l√Ω c√°c file n·∫±m trong d·ª± √°n
            if current_file not in all_project_files_abs:
                continue

            visited.add(current_file)
            
            imports = self._extract_imports_from_file(current_file)
            for imp in imports:
                if imp not in visited:
                    to_visit.append(imp)
        
        return visited

    def _find_usages(self, target_file: Path, all_project_files_abs: Set[Path]) -> Set[Path]:
        """
        T√¨m t·∫•t c·∫£ c√°c file trong d·ª± √°n m√† ƒëang import `target_file`.
        """
        usages: Set[Path] = set()
        self.logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm usages cho file: {target_file.relative_to(self.project_path)}")
        for file_to_scan in all_project_files_abs:
            if file_to_scan == target_file:
                continue
            
            imports = self._extract_imports_from_file(file_to_scan)
            if target_file in imports:
                usages.add(file_to_scan)
        
        return usages

    def merge_dependencies_for_file(self, target_file_str: str):
        """
        Ch·ª©c nƒÉng ch√≠nh: t√¨m dependencies, usages v√† g·ªôp t·∫•t c·∫£ l·∫°i.
        """
        target_file_path = (self.project_path / target_file_str).resolve()

        if not target_file_path.is_file():
            self.logger.error(f"File ƒë√≠ch kh√¥ng t·ªìn t·∫°i: {target_file_path}")
            return

        self.logger.info("B·∫Øt ƒë·∫ßu thu th·∫≠p danh s√°ch file trong d·ª± √°n...")
        all_tracked_files_relative = self.get_tracked_files()
        all_tracked_files_abs = { (self.project_path / f).resolve() for f in all_tracked_files_relative }

        self.logger.info(f"1. T√¨m c√°c file ph·ª• thu·ªôc (dependencies) c·ªßa '{target_file_str}'...")
        dependencies = self._find_dependencies_recursively(target_file_path, all_tracked_files_abs)
        self.logger.info(f" -> T√¨m th·∫•y {len(dependencies)} dependencies (bao g·ªìm c·∫£ file g·ªëc).")

        self.logger.info(f"2. T√¨m c√°c file s·ª≠ d·ª•ng (usages) '{target_file_str}'...")
        usages = self._find_usages(target_file_path, all_tracked_files_abs)
        self.logger.info(f" -> T√¨m th·∫•y {len(usages)} file s·ª≠ d·ª•ng n√≥.")
        
        # G·ªôp t·∫•t c·∫£ k·∫øt qu·∫£ l·∫°i v√† lo·∫°i b·ªè tr√πng l·∫∑p
        all_related_files_abs = dependencies.union(usages)
        
        # Chuy·ªÉn ƒë·ªïi l·∫°i th√†nh ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ g·ªôp file
        all_related_files_relative = sorted([
            str(p.relative_to(self.project_path)).replace('\\', '/') for p in all_related_files_abs
        ])
        
        if not all_related_files_relative:
            self.logger.warning("Kh√¥ng t√¨m th·∫•y file li√™n quan n√†o.")
            return

        self.logger.info(f"T·ªïng c·ªông c√≥ {len(all_related_files_relative)} file li√™n quan. B·∫Øt ƒë·∫ßu g·ªôp...")
        
        output_filename = f"deps-{Path(target_file_str).name.replace('.', '_')}.merged.txt"
        self.merge_specific_files(all_related_files_relative, output_filename=output_filename)            

    def status(self):
        print("\n=== Git File Tracker Status ===")
        print(f"Project Path: {self.project_path}")
        print(f"Output Directory: {self.output_dir.relative_to(self.project_path)}")
        log_file_path = self.output_dir / 'tracker.log'
        meta_file_path = self.metadata_file
        if log_file_path.exists(): print(f"Log File: {log_file_path.relative_to(self.project_path)}")
        if meta_file_path.exists(): print(f"Metadata File: {meta_file_path.relative_to(self.project_path)}")

        last_commit_stored = self.metadata.get('last_commit', 'None')
        print(f"Last Processed Commit: {last_commit_stored}")

        current_git_commit = self.get_current_commit()
        if current_git_commit:
            print(f"Current Git HEAD Commit: {current_git_commit}")
            if current_git_commit != last_commit_stored:
                print("‚ö†Ô∏è  Tr·∫°ng th√°i: C√≥ commit m·ªõi ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi tracker.")
            else:
                print("‚úÖ Tr·∫°ng th√°i: ƒê√£ c·∫≠p nh·∫≠t v·ªõi commit m·ªõi nh·∫•t (theo git commit hash).")
        else:
            print("‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y commit hi·ªán t·∫°i t·ª´ Git.")

        print(f"Total Files in Metadata Hashes: {len(self.metadata.get('file_hashes', {}))}")

        print("\nFile Statistics (t·ª´ metadata['tracked_files']):")
        meta_tracked_files_map = self.metadata.get('tracked_files', {})
        if isinstance(meta_tracked_files_map, dict):
            total_files_in_map = sum(len(files) for files in meta_tracked_files_map.values())
            print(f"  Total files grouped by type in metadata: {total_files_in_map}")
            for file_type, files_list in sorted(meta_tracked_files_map.items()):
                indicator = self._get_file_type_indicator(file_type)
                print(f"  {indicator} {file_type.capitalize()}: {len(files_list)} files")
        else:
            print("  (D·ªØ li·ªáu th·ªëng k√™ file theo lo·∫°i trong metadata kh√¥ng c√≥ ho·∫∑c c√≥ ƒë·ªãnh d·∫°ng kh√¥ng mong mu·ªën)")


        print("\nGenerated Files (excluding log/metadata):")
        generated_count = 0
        if self.output_dir.is_dir():
            for item in sorted(self.output_dir.iterdir()): # S·∫Øp x·∫øp ƒë·ªÉ output nh·∫•t qu√°n
                if item.is_file() and item.name not in ['tracker.log', 'metadata.json']:
                    print(f"  - {item.name}")
                    generated_count +=1
            if generated_count == 0: print("  (Ch∆∞a c√≥ file t·ªïng h·ª£p n√†o ƒë∆∞·ª£c t·∫°o)")
        else: print("  (Th∆∞ m·ª•c output ch∆∞a ƒë∆∞·ª£c t·∫°o)")


def main():
    parser = argparse.ArgumentParser(
        description='Git File Tracker for React Projects',
        formatter_class=argparse.RawTextHelpFormatter # Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng xu·ªëng d√≤ng trong help
    )
    parser.add_argument('--project-path', default=os.getcwd(), help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn d·ª± √°n (m·∫∑c ƒë·ªãnh: th∆∞ m·ª•c hi·ªán t·∫°i)')
    parser.add_argument('--output-dir', default='tracked_files', help='Th∆∞ m·ª•c output (t∆∞∆°ng ƒë·ªëi v·ªõi project-path)')

    action_group = parser.add_argument_group('H√†nh ƒë·ªông')
    action_group.add_argument(
        '--merge',
        nargs='+', # Ch·∫•p nh·∫≠n m·ªôt ho·∫∑c nhi·ªÅu file
        metavar='FILE_PATH',
        help='G·ªôp c√°c file ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh th√†nh files-merged.txt.\n'
             'Argument n√†y s·∫Ω ghi ƒë√® l√™n danh s√°ch fileList ƒë∆∞·ª£c khai b√°o trong code.\n'
             'H√†nh ƒë·ªông n√†y s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n th·ª±c hi·ªán.'
    )
    action_group.add_argument(
        '--merge-dir',
        metavar='DIR_PATH',
        help='(M·ªöI) G·ªôp t·∫•t c·∫£ c√°c file trong m·ªôt th∆∞ m·ª•c v√† c√°c th∆∞ m·ª•c con c·ªßa n√≥.\n'
             'C√°c file trong node_modules, .git, v.v. s·∫Ω ƒë∆∞·ª£c b·ªè qua.\n'
             'V√≠ d·ª•: --merge-dir src/components'
    )
    # ===== START: ARGUMENT M·ªöI CHO ERROR DICT =====
    action_group.add_argument(
        '--merge-error',
        metavar='ERROR_ID',
        help='(M·ªöI) G·ªôp c√°c file li√™n quan ƒë·∫øn m·ªôt ID l·ªói t·ª´ errorDict.json.\n'
             'C·∫ßn cung c·∫•p ƒë∆∞·ªùng d·∫´n ƒë·∫øn errorDict.json qua --error-dict-path.\n'
             'V√≠ d·ª•: --merge-error 9.2'
    )
    parser.add_argument( # Th√™m argument ri√™ng cho ƒë∆∞·ªùng d·∫´n errorDict
        '--error-dict-path',
        default='errorDict.json', # M·∫∑c ƒë·ªãnh t√¨m file errorDict.json ·ªü th∆∞ m·ª•c ch·∫°y script
        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file errorDict.json (m·∫∑c ƒë·ªãnh: errorDict.json)'
    )
    # ===== END: ARGUMENT M·ªöI CHO ERROR DICT =====
    action_group.add_argument('--initial-scan', action='store_true', help='Th·ª±c hi·ªán scan ban ƒë·∫ßu to√†n b·ªô d·ª± √°n')
    action_group.add_argument('--check-update', action='store_true', help='Ki·ªÉm tra v√† c·∫≠p nh·∫≠t thay ƒë·ªïi t·ª´ commit m·ªõi nh·∫•t')
    action_group.add_argument('--status', action='store_true', help='Hi·ªÉn th·ªã tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa tracker')
    action_group.add_argument('--create-hook', action='store_true', help='T·∫°o/c·∫≠p nh·∫≠t git post-commit hook')
    
    action_group.add_argument(
        '--merge-deps',
        metavar='FILE_PATH',
        help='(M·ªöI) T√¨m v√† g·ªôp m·ªôt file c√πng t·∫•t c·∫£ c√°c file ph·ª• thu·ªôc (dependencies) v√† c√°c file s·ª≠ d·ª•ng n√≥ (usages).'
    )

    args = parser.parse_args()

    global fileList # Khai b√°o ƒë·ªÉ c√≥ th·ªÉ thay ƒë·ªïi bi·∫øn to√†n c·ª•c
    if args.merge:
        fileList = args.merge # Ghi ƒë√® fileList n·∫øu --merge ƒë∆∞·ª£c d√πng

    project_path_resolved = Path(args.project_path).resolve()
    tracker = GitFileTracker(str(project_path_resolved), args.output_dir)

    # ∆Øu ti√™n c√°c h√†nh ƒë·ªông merge
    if fileList: # X·ª≠ l√Ω --merge ho·∫∑c fileList to√†n c·ª•c
        tracker.merge_specific_files(fileList)
    elif args.merge_dir:
        tracker.merge_directory_files(args.merge_dir)
    # ===== START: X·ª¨ L√ù H√ÄNH ƒê·ªòNG MERGE THEO ERROR =====
    elif args.merge_error:
        error_dict_data = tracker.load_error_dict(args.error_dict_path)
        if error_dict_data:
            tracker.merge_files_by_error_id(error_dict_data, args.merge_error)
        else:
            # logger ƒë√£ b√°o l·ªói r·ªìi, c√≥ th·ªÉ kh√¥ng c·∫ßn print th√™m
            pass
    # ===== END: X·ª¨ L√ù H√ÄNH ƒê·ªòNG MERGE THEO ERROR =====
    elif args.merge_deps:
        tracker.merge_dependencies_for_file(args.merge_deps)
    elif args.initial_scan:
        tracker.initial_scan()
    elif args.check_update:
        tracker.check_and_update()
    elif args.status:
        tracker.status()
    elif args.create_hook:
        tracker.create_git_hook()
    else:
        # H√†nh ƒë·ªông m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ c·ªù n√†o ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh V√Ä fileList r·ªóng
        print("Kh√¥ng c√≥ h√†nh ƒë·ªông n√†o ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh v√† fileList r·ªóng. Hi·ªÉn th·ªã tr·∫°ng th√°i.")
        print("S·ª≠ d·ª•ng --help ƒë·ªÉ xem c√°c t√πy ch·ªçn.")
        tracker.status()

if __name__ == '__main__':
    main()




# parser.add_argument('--project-path', default=os.getcwd(), help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn d·ª± √°n (m·∫∑c ƒë·ªãnh: th∆∞ m·ª•c hi·ªán t·∫°i)')
# parser.add_argument('--output-dir', default='tracked_files', help='Th∆∞ m·ª•c output (t∆∞∆°ng ƒë·ªëi v·ªõi project-path)')

# action_group.add_argument('--initial-scan', action='store_true', help='Th·ª±c hi·ªán scan ban ƒë·∫ßu to√†n b·ªô d·ª± √°n')
# action_group.add_argument('--check-update', action='store_true', help='Ki·ªÉm tra v√† c·∫≠p nh·∫≠t thay ƒë·ªïi t·ª´ commit m·ªõi nh·∫•t')
# action_group.add_argument('--status', action='store_true', help='Hi·ªÉn th·ªã tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa tracker')
# action_group.add_argument('--create-hook', action='store_true', help='T·∫°o/c·∫≠p nh·∫≠t git post-commit hook')

# action_group.add_argument(
#     '--merge',
#     nargs='+', # Ch·∫•p nh·∫≠n m·ªôt ho·∫∑c nhi·ªÅu file
#     metavar='FILE_PATH',
#     help='G·ªôp c√°c file ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh th√†nh files-merged.txt.\n'
#             'Argument n√†y s·∫Ω ghi ƒë√® l√™n danh s√°ch fileList ƒë∆∞·ª£c khai b√°o trong code.\n'
#             'H√†nh ƒë·ªông n√†y s·∫Ω ƒë∆∞·ª£c ∆∞u ti√™n th·ª±c hi·ªán.'
# )

# action_group.add_argument(
#     '--merge-dir',
#     metavar='DIR_PATH',
#     help='(M·ªöI) G·ªôp t·∫•t c·∫£ c√°c file trong m·ªôt th∆∞ m·ª•c v√† c√°c th∆∞ m·ª•c con c·ªßa n√≥.\n'
#             'C√°c file trong node_modules, .git, v.v. s·∫Ω ƒë∆∞·ª£c b·ªè qua.\n'
#             'V√≠ d·ª•: --merge-dir src/components'
# )

# action_group.add_argument(
#     '--merge-error',
#     metavar='ERROR_ID',
#     help='(M·ªöI) G·ªôp c√°c file li√™n quan ƒë·∫øn m·ªôt ID l·ªói t·ª´ errorDict.json.\n'
#             'C·∫ßn cung c·∫•p ƒë∆∞·ªùng d·∫´n ƒë·∫øn errorDict.json qua --error-dict-path.\n'
#             'V√≠ d·ª•: --merge-error 9.2'
# )

# parser.add_argument( # Th√™m argument ri√™ng cho ƒë∆∞·ªùng d·∫´n errorDict
#     '--error-dict-path',
#     default='errorDict.json', # M·∫∑c ƒë·ªãnh t√¨m file errorDict.json ·ªü th∆∞ m·ª•c ch·∫°y script
#     help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file errorDict.json (m·∫∑c ƒë·ªãnh: errorDict.json)'
# )